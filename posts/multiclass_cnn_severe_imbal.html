<div class="blog-post">
  <h2 class="title">Project: Multi-Class CNN with Severe Class Imbalance from Combined Datasets</h2>
  <p>Posted April 20, 2025 by Ian Ryan</p>
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  <section id="intro">
  <h2>âœ¨ Introduction / TL;DR</h2>
  <p>
    I took on the challenge of building a multi-class Convolutional Neural Network from scratch using PyTorch â€” with a twist. Instead of using balanced or curated datasets, I intentionally created an extreme case of class imbalance by merging two different datasets. One of the 18 classes makes up 50% of the images, while the remaining 17 classes split the other half.
  </p>
  <p>
    Since graduating, Iâ€™ve been refining this project and correcting mistakes that were present in the original capstone version. One major issue I uncovered was that I had incorrectly combined the datasets, which resulted in misclassified images and even some completely empty classes â€” making all the original training results invalid. I committed to debugging everything: redundant code, mismatched variable names, inconsistent transforms, and broken visualizations. Once the pipeline was stable and fully functional, I moved on to implementing upgrades across the board â€” from dataset engineering and augmentation to better loss strategies and metrics.
  </p>
  <p>
    <strong>The combined dataset includes:</strong><br>
    - <strong><a href="https://universe.roboflow.com/pascal-to-yolo-8yygq/inria-person-detection-dataset" target="_blank">INRIA Person Detection Dataset</a></strong> (Roboflow Universe) by user <strong>Pascal to Yolo</strong>: 902 images of class <code>person</code>, manually split 70/20/10 for train/val/test.<br>
    - <strong><a href="https://www.kaggle.com/datasets/iamsouravbanerjee/animal-image-dataset-90-different-animals/data" target="_blank">Animal Image Dataset</a></strong> (Kaggle) by user <strong>iamsouravbanerjee</strong>: 17 cherry-picked animal classes from their 90 total, totaling 902 images, also split 70/20/10.
  </p>
  <p>
    Merged, this totals <strong>1,804 images</strong> across 18 classes with heavy imbalance: 1 class dominates half the dataset. Everything is now preprocessed locally and trained fully from scratch.
  </p>
  <p>
    <strong>After tuning and implementing synthetic augmentation, I was able to achieve:</strong><br>
    - <strong>70%+</strong> accuracy on training, validation, and test sets<br>
    - <strong>Macro F1-Score</strong> ranging from <strong>0.40 to 0.60</strong><br>
    - All without using any pretrained models
  </p>
  <p>
    It became a personal challenge â€” one I used to reinforce core deep learning concepts under tougher, more realistic conditions. Itâ€™s been a fun dissection thatâ€™s helped me growâ€”improving my skills, decision making, and overall approach to building machine learning pipelines, while continuing my journey of finding employment after graduating in December.
  </p>
</section>

  
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">

  <section id="hardware-comparison">
    <h2>â•°â•´ 1. ğŸ’» From Cloud to Couch: Rebuilding the Pipeline on Local Hardware</h2>
    <p>
      The original version of this project â€” my capstone â€” was built and trained on an <strong>AWS SageMaker instance</strong>, which provided powerful GPU resources for prototyping and experimentation. But for this final version, I transitioned everything to run on my <strong>personal laptop</strong>.
    </p>

    <h4>Why the Shift?</h4>
    <p>
      <em>Financial constraints.</em> I couldnâ€™t justify continuing to pay for cloud computing, so I adapted the pipeline to work within the limits of my local machine. The switch forced me to be more deliberate about memory usage â€” particularly <strong>CUDA VRAM</strong> â€” and made me pay closer attention to cleanup, caching, and resource management throughout the notebook.
    </p>

    <div class="env-comparison">
  <div class="env-box">
    <h3>â˜ï¸ Capstone Environment: AWS SageMaker</h3>
    <ul>
      <li><strong>Platform:</strong> AWS SageMaker Jupyter Notebook</li>
      <li><strong>Instance Type:</strong> <code>ml.g4dn.2xlarge</code></li>
      <li><strong>GPU:</strong> NVIDIA T4 Tensor Core (16 GB GDDR6)</li>
      <li><strong>CPU:</strong> Intel Xeon Family</li>
      <li><strong>RAM:</strong> 32 GB System Memory</li>
      <li><strong>Storage:</strong> 5 GB EBS Volume</li>
      <li><strong>OS:</strong> Amazon Linux 2 with JupyterLab 3</li>
    </ul>
  </div>

  <div class="env-box">
    <h3>ğŸ§  Final Version Environment: My Laptop</h3>
    <ul>
      <li><strong>Model:</strong> Alienware m15 R7</li>
      <li><strong>OS:</strong> Pop!_OS 22.04 LTS</li>
      <li><strong>CPU:</strong> AMD Ryzen 7 6800H (16 threads @ 4.78 GHz)</li>
      <li><strong>RAM:</strong> 16 GB DDR5</li>
      <li><strong>GPU:</strong> NVIDIA GeForce RTX 3060 Mobile (6 GB VRAM)</li>
      <li><strong>Secondary GPU:</strong> Integrated AMD Radeon Graphics</li>
      <li><strong>CUDA Version:</strong> 12.8</li>
      <li><strong>Kernel:</strong> 6.12.10-76061203-generic</li>
    </ul>
  </div>
</div>


    <h4>ğŸ“Š GPU Status (via <code>nvidia-smi</code>):</h4>
    <pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 570.124.04    Driver Version: 570.124.04    CUDA Version: 12.8   |
|--------------------------+---------------------------+----------------------|
| GPU  Name        Memory  | Bus-Id        GPU-Util    | Memory-Usage         |
| 0    RTX 3060    6 GB    | 0000:01:00.0  0%           | 15 MiB / 6144 MiB   |
+-----------------------------------------------------------------------------+</code></pre>

    <h4>ğŸ§® Memory Snapshot (via <code>free -h</code>):</h4>
    <pre><code>Mem:   14Gi   used: 5.1Gi   free: 1.2Gi   buff/cache: 8.6Gi
Swap:  18Gi   used: 0B      free: 18Gi</code></pre>

    <h3>âš™ï¸ What Changed?</h3>
    <ul>
      <li>ğŸ” <strong>Cached Computation:</strong> I saved mean/std stats to disk to avoid recomputation on kernel restarts.</li>
      <li>ğŸ§¹ <strong>VRAM Management:</strong> I routinely cleared CUDA memory using <code>torch.cuda.empty_cache()</code> and <code>gc.collect()</code>.</li>
      <li>ğŸ§± <strong>Lightweight Pipeline:</strong> I restructured the notebook to be more memory-efficient and modular.</li>
    </ul>
  </section>
  
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  
  <section id="dataset-cleanup">
    <h2>â•°â•´ 2. ğŸ§¹ Dataset Cleanup, Refactoring & Fixing Merge Issues</h2>

    <p>
      In the early stages of the project, I uncovered several inconsistencies and bugs in the dataset preparation pipeline.
      These werenâ€™t just minor issuesâ€”they had a real impact on model performance, evaluation, and even basic functionality.
      Before I could get any reliable training off the ground, I had to carefully debug and refactor how my datasets were being
      merged, labeled, and verified. Here's what went wrong, how I tracked it down, and the steps I took to fix it.
    </p>

    <h3>ğŸ§© Issue #1: Merging Datasets Broke Class Indexing</h3>

    <h4>âŒ Problem:</h4>
    <p>This project involved merging two datasets:</p>
    <ul>
      <li><strong>INRIA:</strong> One class (<code>person</code>) with <code>train/</code>, <code>valid/</code>, <code>test/</code> splits</li>
      <li><strong>Animal Dataset:</strong> 17 animal classes (e.g., badger, zebra, etc.) with the same structure</li>
    </ul>
    <p>PyTorchâ€™s <code>ImageFolder</code> assigns indices alphabetically, so both datasets started at 0. When combined using <code>ConcatDataset</code>, this caused:</p>
    <ul>
      <li>Label collisions (e.g., <code>person</code> and <code>badger</code> both mapped to index 0)</li>
      <li>Misclassified training examples</li>
      <li>Broken evaluation logic</li>
    </ul>

    <h4>ğŸ’¡ Fix:</h4>
    <p>I applied a label offset before combining the datasets:</p>

    <pre><code>offset = len(inria_classes)
animal_label = original_animal_label + offset</code></pre>

    <p>âœ”ï¸ This made the label space consistent and eliminated overlap.</p>

    <hr style="width: 60%; margin: 40px auto; border: none; border-top: 1px solid #c0c0c0;">


    <h3>ğŸ§¹ Issue #2: Empty or Mismatched Class Folders</h3>

    <h4>âŒ Problem:</h4>
    <ul>
      <li>Some folders were completely empty</li>
      <li>Others had mismatched folder names vs. class labels</li>
      <li>Missing images caused underrepresented classes</li>
    </ul>

    <h4>ğŸ’¡ Fix:</h4>
    <p>I built a dataset structure verifier that checked folder contents per split:</p>

    <pre><code>def verify_combined_dataset_structure(base_path, class_names):
    issues = False
    for split in splits:
        split_path = os.path.join(base_path, split)
        for idx, class_name in enumerate(class_names):
            class_dir = os.path.join(split_path, class_name)
            if not os.path.exists(class_dir):
                print(f"Missing: {class_dir}")
                issues = True
                continue
            if not os.listdir(class_dir):
                print(f"Empty: {class_dir}")
                issues = True
    if not issues:
        print("âœ… All folders verified.")</code></pre>

    <p>âœ… I now run this check before every training session.</p>

    <hr style="width: 60%; margin: 40px auto; border: none; border-top: 1px solid #c0c0c0;">

    <h3>ğŸ§¼ Issue #3: Code Redundancy & Transform Confusion</h3>

    <h4>âŒ Problem:</h4>
    <ul>
      <li>Redundant transforms and inconsistent variable names</li>
      <li>Scattered <code>ImageFolder</code> and <code>DataLoader</code> logic</li>
    </ul>

    <h4>ğŸ’¡ Fix:</h4>
    <p>I consolidated the transforms into three core configs:</p>
    <ul>
      <li><code>train_transforms</code></li>
      <li><code>val_test_transforms</code></li>
      <li><code>resize_transforms</code></li>
    </ul>
    <p>âœ… All dataset loading now happens <em>after</em> the merge and cleanup.</p>

    <hr style="width: 60%; margin: 40px auto; border: none; border-top: 1px solid #c0c0c0;">

    <h3>ğŸ“Œ My Takeaway</h3>
    <p>When merging datasets:</p>
    <ul>
      <li>âœ… Offset class indices manually</li>
      <li>âœ… Verify all folders and label consistency</li>
      <li>âœ… Persist label mappings (e.g., <code>class_names.json</code>)</li>
      <li>âœ… Merge before applying transforms or creating loaders</li>
    </ul>
    <p><strong>ğŸ§  Dataset hygiene saves hours of downstream debugging.</strong></p>
  </section>
  
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  
  <section id="save-combined-dataset">
  <h2>â•°â•´ 3. ğŸ§© Saving the Merged Dataset & Setting a Restart Checkpoint</h2>
  <p>
    After verifying that my combined dataset was structured correctly and balanced across splits, I wanted to:
  </p>
  <ul>
    <li>ğŸ’¾ Save it in a reproducible format</li>
    <li>ğŸ§¹ Clean up memory by deleting the originals</li>
    <li>ğŸ›¡ï¸ Avoid having to re-download or re-process if I restarted my notebook kernel</li>
  </ul>
  <p>
    To accomplish this, I wrote a <code>save_combo_dataset()</code> function that:
  </p>
  <ul>
    <li>Saves all image data in <code>data/comboset/</code> with the correct class structure</li>
    <li>Writes <code>class_names.json</code> (a list of class labels)</li>
    <li>Writes <code>dataset_stats.json</code> (split sizes, class distribution, total images, label offset info)</li>
  </ul>

  <pre><code>save_combo_dataset(train_dataset, val_dataset, test_dataset, overwrite=True)</code></pre>

  <h3>ğŸ—‘ï¸ Deleting Original Datasets to Save Space</h3>
  <p>
    Immediately afterward, I deleted the <code>animal</code> and <code>INRIA-pdd</code> folders to free up disk and VRAM. I even added a redundant "nuke-from-orbit" check just in case any stray folders stuck around:
  </p>

  <pre><code class="language-python"># Paths to original datasets
animal_path = "/home/ian/Desktop/Cap/data/animal"
inria_path = "/home/ian/Desktop/Cap/data/INRIA-pdd"

# Delete folders
shutil.rmtree(animal_path, ignore_errors=True)
shutil.rmtree(inria_path, ignore_errors=True)

# Clear CUDA memory and Python garbage
torch.cuda.empty_cache()
torch.cuda.ipc_collect()
gc.collect()

# Double delete, just to be sure
if os.path.exists(animal_path):
    shutil.rmtree(animal_path, ignore_errors=True)

if os.path.exists(inria_path):
    shutil.rmtree(inria_path, ignore_errors=True)

print("Removed animal and INRIA-pdd folders. CUDA memory cleared.")</code></pre>

  <h3>ğŸ” Post-Restart Checkpoint</h3>
  <p>
    To protect against Jupyter kernel crashes or restarts, I created a recovery cell that reloads the class names from <code>class_names.json</code>. This meant I could continue training or evaluation without having to re-run earlier cells:
  </p>

  <pre><code class="language-python"># Post restart resume training/eval
if 'combined_classes' not in globals():
    combo_path = 'data/comboset'
    with open(os.path.join(combo_path, 'class_names.json'), 'r') as f:
        combined_classes = json.load(f)

out_classes = len(combined_classes)
print("Restored class names:", combined_classes)
print("Number of output classes:", out_classes)</code></pre>

  <p><strong>âœ… This became a safe and efficient checkpoint to restart from, without having to re-download or re-process data.</strong></p>

  <h4>ğŸ’¡ What This Enabled</h4>
  <ul>
    <li>âœ… <strong>Space Efficiency</strong>: Removed 2 full datasets from disk</li>
    <li>âœ… <strong>VRAM Recovery</strong>: Prevented CUDA crashes during later training</li>
    <li>âœ… <strong>Modular Workflow</strong>: Could pause/resume without redoing everything</li>
    <li>âœ… <strong>Cleaner Notebook State</strong>: Reduced bugs and clutter from lingering data</li>
  </ul>
</section>

  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  <section id="global-config">
  <h2>â•°â•´ 4. ğŸŒ Global Configuration & Dataset Verification</h2>
  <p>
    After merging the two datasets into <code>comboset</code> and saving a persistent checkpoint, I needed a reliable way to resume my work â€” especially after restarting the kernel or clearing memory. This section defined global paths, ensured consistency across the codebase, and validated that the dataset structure was correct before any model training began.
  </p>

  <h3>ğŸ“ Global Paths & Helpers</h3>
  <p>
    To avoid path hardcoding or redefinition errors, I introduced a set of consistent global paths and a helper function to dynamically grab train/val/test directories from a given base path:
  </p>

  <pre><code class="language-python"># Global config
if 'combo_path' not in globals():
    combo_path = 'data/comboset'

splits = ['train', 'valid', 'test']

# Helper function to extract split paths
def get_split_paths(base_path):
    return (
        os.path.join(base_path, 'train'),
        os.path.join(base_path, 'valid'),
        os.path.join(base_path, 'test')
    )

train_path, val_path, test_path = get_split_paths(combo_path)</code></pre>

  <h3>ğŸ” Validating Folder Structure</h3>
  <p>
    Before moving forward, I needed a guarantee that every class was represented across all splits and that no images were silently dropped. So I built a custom verification script:
  </p>

  <pre><code class="language-python">def verify_combined_dataset_structure(base_path, class_names):
    issues = False
    for split in splits:
        split_path = os.path.join(base_path, split)
        print(f"\nChecking split: {split}/")

        for idx, class_name in enumerate(class_names):
            class_dir = os.path.join(split_path, class_name)
            if not os.path.exists(class_dir):
                print(f"â”œâ”€â”€ Missing class folder: {class_dir}")
                issues = True
                continue

            files = os.listdir(class_dir)
            connector = "â””â”€â”€" if idx == len(class_names) - 1 else "â”œâ”€â”€"
            if not files:
                print(f"{connector} Empty class folder: {class_dir}")
                issues = True
            else:
                print(f"{connector} {class_name}: {len(files)} images")

    print("\nNo issues." if not issues else "\nIssues found in folder structure.")
# Run check
verify_combined_dataset_structure(combo_path, combined_classes)</code></pre>

  <h4>âœ… Output:</h4>
  <pre><code>Checking split: train/
â”œâ”€â”€ person: 632 images
â”œâ”€â”€ badger: 38 images
â”œâ”€â”€ cat: 38 images
...
â””â”€â”€ zebra: 37 images

Checking split: valid/
â”œâ”€â”€ person: 180 images
â”œâ”€â”€ badger: 11 images
...
â””â”€â”€ zebra: 10 images

Checking split: test/
â”œâ”€â”€ person: 90 images
â”œâ”€â”€ badger: 5 images
...
â””â”€â”€ zebra: 6 images

No issues.
</code></pre>

  <blockquote>
    <strong>ğŸ§  Insight:</strong> This script became a non-negotiable checkpoint before any modeling phase â€” catching issues like misnamed folders, dropped files, or missing images that would otherwise break training silently.
  </blockquote>
    <h3>ğŸ“Š Class Distribution Visualization</h3>
  <p>
    To supplement the verification script, I used a JSON-driven visualization utility to confirm class balance across each split.
    This pulled from <code>dataset_stats.json</code>, which was generated after saving the combined dataset structure.
  </p>

  <pre><code class="language-python">def visualize_dataset_stats(stats_path, save_plots=False, show_pie=False):
    with open(stats_path, 'r') as f:
        stats = json.load(f)

    print(f"Total images: {stats['total_images']}")
    print(f"Split percentages: {stats['splits_percentage']}")
    print(f"Label offset applied to: {stats['offset_applied_to']} (Offset: {stats['offset_value']})\\n")

    for split_name, split_stats in stats["class_distribution"].items():
        class_names = list(split_stats.keys())
        class_counts = list(split_stats.values())

        total_images = np.sum(class_counts)
        percentages = (class_counts / total_images) * 100

        # Warn if any class exceeds 50% or is less than 5%
        for i, pct in enumerate(percentages):
            if pct > 50:
                print(f"âš ï¸ '{class_names[i]}' is over 50% of {split_name} set ({pct:.1f}%)")
            elif pct < 5:
                print(f"âš ï¸ '{class_names[i]}' is under 5% of {split_name} set ({pct:.1f}%)")

        # Create bar chart per split
        plt.figure(figsize=(10, 4))
        bars = plt.bar(class_names, class_counts, color='skyblue')
        plt.axhline(np.mean(class_counts), color='green', linestyle='--', label='Mean')
        plt.axhline(np.median(class_counts), color='orange', linestyle='--', label='Median')
        plt.title(f"Class Distribution in {split_name.upper()} Set")
        plt.xticks(rotation=45, ha='right')
        plt.legend()

        for bar, pct in zip(bars, percentages):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2, height + 1, f"{pct:.1f}%", ha='center', fontsize=8)

        plt.tight_layout()
        plt.grid(True, axis='y', linestyle='--', alpha=0.5)
        if save_plots:
            plt.savefig(f"{split_name}_distribution.png")
        plt.show()</code></pre>

  <h4>ğŸ“Œ Example Output:</h4>
  <pre><code>ğŸš¨ Class imbalance detected in TRAIN set:
âš ï¸ 'person' is over 50% of train set (50.1%)
âš ï¸ 'zebra' is under 5% of train set (2.9%)
...
</code></pre>

  <h4>Class Distribution Graphs:</h4>
  <div style="text-align: center; margin-top: 30px;">
    <h4>ğŸ“Š Class Distribution: Train Set</h4>
    <img src="pictures/train_class_dist.png" alt="Class Distribution in Train Set" style="max-width: 100%; height: auto;">
  </div>

  <div style="text-align: center; margin-top: 30px;">
    <h4>ğŸ“Š Class Distribution: Validation Set</h4>
    <img src="pictures/val_class_dist.png" alt="Class Distribution in Validation Set" style="max-width: 100%; height: auto;">
  </div>

  <div style="text-align: center; margin-top: 30px;">
    <h4>ğŸ“Š Class Distribution: Test Set</h4>
    <img src="pictures/test_class_dist.png" alt="Class Distribution in Test Set" style="max-width: 100%; height: auto;">
  </div>

	<h4>ğŸ§  Why This Matters</h4>
  <blockquote>
    <strong>ğŸ§  Insight:</strong> I overlooked serious structural issues in my capstone and didnâ€™t realize them until after training. This time, I combined folder validation with visual distribution feedback â€” making sure every class was present, populated, and realistically proportioned before training began.
  </blockquote>
</section>
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  <section id="mean-std-normalization">
  <h2>â•°â•´ 5. ğŸ§® Mean & Std Normalization: Precision, Consistency, and Caching</h2>
  <p>
    Normalizing image datasets using channel-wise mean and standard deviation is a critical step for stable and efficient training â€” especially for CNNs. In my original capstone, I implemented a simple <code>calculate_mean_std()</code> function, but it had some rough edges. By the time I reached the final iteration, I had reworked it entirely for both accuracy and reusability.
  </p>

<div class="meanstd-comparison">
  <div class="window meanstd-box">
    <div class="title-bar">
      <div class="title-bar-text">Capstone Version: Single-Pass, Underestimated Variance</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <pre><code class="language-python">def calculate_mean_std(loader):
    mean = 0.
    std = 0.
    total_images = 0

    for data, _ in loader:
        batch_samples = data.size(0)
        total_images += batch_samples
        data = data.view(batch_samples, data.size(1), -1)
        mean += data.mean(2).sum(0)
        std += data.std(2).sum(0)

    mean /= total_images
    std /= total_images
    return mean, std
    </code></pre>
      <p>This version loops once, but underestimates variance due to image-based averaging.</p>
      <pre><code>
Calculated Mean: tensor([0.4713, 0.4595, 0.4212])
Calculated Std:  tensor([0.2065, 0.2032, 0.2050])
</code></pre>
    </div>
  </div>

  <div class="window meanstd-box">
    <div class="title-bar">
      <div class="title-bar-text">Final Version: Double-Pass for True Pixel-Wise Accuracy</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <pre><code class="language-python">def compute_mean_std(loader):
    n_channels = 3
    mean = torch.zeros(n_channels)
    std = torch.zeros(n_channels)
    n_pixels = 0

    for data, _ in loader:
        batch_samples = data.size(0)
        data = data.view(batch_samples, data.size(1), -1)
        mean += data.sum(2).sum(0)
        n_pixels += data.size(2) * batch_samples

    mean /= n_pixels

    for data, _ in loader:
        batch_samples = data.size(0)
        data = data.view(batch_samples, data.size(1), -1)
        std += ((data - mean.view(1, -1, 1)) ** 2).sum(2).sum(0)

    std = torch.sqrt(std / n_pixels)
    return mean, std
    </code></pre>
      <p>This version does a full two-pass mean & std with pixel-based precision.</p>
      <pre><code>
Computed Mean: tensor([0.4712, 0.4595, 0.4211])
Computed Std:  tensor([0.2522, 0.2475, 0.2572])
</code></pre>
    </div>
  </div>
</div>

<div class="window meanstd-box">
  <div class="title-bar">
    <div class="title-bar-text">ğŸ’¾ Added Caching</div>
    <div class="title-bar-controls">
      <button aria-label="Minimize"></button>
      <button aria-label="Close"></button>
    </div>
  </div>
  <div class="window-body">
    <pre><code class="language-python">def get_or_compute_mean_std(loader, save_path='mean_std.pt'):
    if os.path.exists(save_path):
        stats = torch.load(save_path)
        print("Load mean/std via file.")
        return stats['mean'], stats['std'], "loaded"
    else:
        mean, std = compute_mean_std(loader)
        torch.save({'mean': mean, 'std': std}, save_path)
        print("Computed & saved mean/std.")
        return mean, std, "computed"
</code></pre>
  </div>
</div>

  <p>This function automatically loads <code>mean_std.pt</code> if available â€” or computes it and caches the result. This caching logic reduces runtime and prevents redundant computation during repeated training sessions.</p>

  <blockquote>
    ğŸ“Œ <strong>Used later in:</strong> <code>Transforms & Loaders</code> cell â€” ensuring that the normalization applied to the dataset matches across training, validation, and testing every time.
  </blockquote>

  <h3>ğŸ§  Summary: Why This Change Mattered</h3>
  <table>
    <thead>
      <tr>
        <th>Version</th>
        <th>Passes</th>
        <th>Accuracy</th>
        <th>Pixel-Based</th>
        <th>Caches Results</th>
        <th>Notes</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Capstone</td>
        <td>1</td>
        <td>Approximate</td>
        <td>âŒ</td>
        <td>âŒ</td>
        <td>Underestimates std; fast but biased</td>
      </tr>
      <tr>
        <td>Final</td>
        <td>2</td>
        <td>Precise</td>
        <td>âœ…</td>
        <td>âœ…</td>
        <td>Used total pixel count + caching</td>
      </tr>
    </tbody>
  </table>

  <p>
    The capstone version underestimated variance, which led to unstable learning and poorer generalization. The improved version produced better-normalized data, faster convergence, and more consistent results â€” with the bonus of avoiding unnecessary recomputation.
  </p>
</section>
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  <section id="transforms-loaders">
  <h2>â•°â•´ 6. ğŸŒ€ Transforms & Loaders: From Augmentation to Augmented Intelligence</h2>
  <p>
    This section of the pipeline is where I cleaned up some of the most impactful mistakes from the capstone version.
    That included removing duplicate logic, fixing bugs in the data loading flow, and introducing upgrades like
    MixUp augmentation and smarter sampling. These changes made training more stable, and the code easier to maintain and debug.
  </p>

  <p>
    One of the most meaningful upgrades was implementing <strong>synthetic augmentation</strong> using the
    <code>MixUp</code> technique, which blends image pairs and interpolates their labels.
    This technique helps improve generalization and reduces the likelihood of overfitting â€”
    particularly important in scenarios with long-tail class imbalance.
  </p>

  <h3>ğŸ” Key Upgrades Made:</h3>
  <ul>
    <li><strong>ğŸ§¼ Cleaned Redundancies & Logical Errors:</strong>
      <ul>
        <li>Removed duplicated or unused transform blocks like <code>combo_transformation</code></li>
        <li>Eliminated conflicting variables like <code>val_loader</code> vs <code>valid_loader</code></li>
        <li>Reorganized transform usage to avoid misalignment in training vs evaluation</li>
      </ul>
    </li>
    <li><strong>ğŸ§ª Introduced <code>MixUpDataset</code> Wrapper:</strong>
      <ul>
        <li>Replaces raw samples with convex combinations of two samples</li>
        <li>Outputs soft labels using <code>F.one_hot()</code> interpolation</li>
      </ul>
    </li>
    <li><strong>ğŸ§² Added <code>WeightedRandomSampler</code>:</strong>
      <ul>
        <li>Balances class representation by adjusting sampling probabilities</li>
        <li>Ensures minority classes appear more frequently in each epoch</li>
      </ul>
    </li>
    <li><strong>ğŸ§® Smarter Batching & Output Management:</strong>
      <ul>
        <li>Dynamically counted class instances from <code>train_dataset.imgs</code></li>
        <li>Calculated output class size automatically</li>
        <li>Batch shape inspection added for sanity checks</li>
      </ul>
    </li>
  </ul>
</section>

  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  <section id="class-weighting">

  <h2>â•°â•´ 7. ğŸ§® Comparing Class Weighting Strategies</h2>
<p>
  Another huge improvement I made to the pipeline was changing how I calculated class weights. I considered testing an approach called 
  <a href="https://arxiv.org/abs/1901.05555" target="_blank" style="color: blue;">
    Class-Balanced Loss Based on Effective Number of Samples
  </a>, 
  introduced in a research paper by <strong>Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie</strong>. It was specifically developed to address dataset imbalance challenges.
  However, I ultimately opted for the more straightforward <strong>Log-Scaled Inverse Frequency</strong> method, replacing my original use of sklearnâ€™s Balanced Class Weights.
  The difference in the resulting class weights (see below) was substantial.
</p>

  <h3>ğŸ“¦ Reminde - Hereâ€™s what the data looked like after merging:</h3>
  <ul>
    <li><strong>train/</strong> â†’ <code>person</code>: <strong>632</strong> images | each animal class: <strong>~37â€“38</strong></li>
    <li><strong>valid/</strong> â†’ <code>person</code>: <strong>180</strong> images | each animal class: <strong>~10â€“11</strong></li>
    <li><strong>test/ </strong>  â†’ <code>person</code>: <strong>90</strong> images  | each animal class: <strong>~5â€“6</strong></li>
  </ul>
  <p>
    This created a <strong>severely imbalanced dataset</strong> where <code>person</code> made up ~50% of the total images. This imbalance was intentional to simulate realism but required careful loss weighting to prevent the model from ignoring minority classes.
  </p>
  <p>
    In my capstone, I used sklearnâ€™s <code>compute_class_weight('balanced')</code>. In practice, this method overcompensated:
  </p>
<div class="meanstd-comparison">
  <div class="window meanstd-box">
    <div class="title-bar">
      <div class="title-bar-text">ğŸ§ª Capstone Strategy: Balanced Weights</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <p><strong>Formula:</strong></p>
      <pre><code>w_c = (N) / (C Ã— n_c)</code></pre>
      <ul>
        <li><code>w_c</code>: weight for class c</li>
        <li><code>N</code>: total number of samples</li>
        <li><code>C</code>: number of classes</li>
        <li><code>n_c</code>: samples in class c</li>
      </ul>
      <p>Used <code>compute_class_weight('balanced')</code> â€” but it overcompensated:</p>
      <pre><code>
# Capstone Weights
tensor([
  1.8480, 1.8480, 1.8480, 1.8979, 1.8979, 1.8979,
  1.8979, 1.8979, 1.8979, 0.1111, 1.8979, 1.8979,
  1.8979, 1.8979, 1.8979, 1.8979, 1.8979, 1.8979
], device='cuda:0')</code></pre>
      <p>ğŸ›‘ <code>person</code> got 0.1111 vs. ~1.89 for animals â†’ 17Ã— imbalance caused poor generalization.</p>
    </div>
  </div>

  <div class="window meanstd-box">
    <div class="title-bar">
      <div class="title-bar-text">ğŸ§  Final Strategy: Log-Scaled Weights</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <p><strong>Formula:</strong></p>
      <pre><code>w_c = log(1 + N / n_c)</code></pre>
      <ul>
        <li><code>w_c</code>: weight for class c</li>
        <li><code>N</code>: total samples</li>
        <li><code>n_c</code>: samples in class c</li>
      </ul>
      <p>Later replaced with smoother <code>log(1 + N / n_c)</code> weights:</p>
      <pre><code>
# Final Weights:
tensor([
  0.0574, 0.0574, 0.0574, 0.0578, 0.0578, 0.0578,
  0.0578, 0.0578, 0.0578, 0.0179, 0.0578, 0.0578,
  0.0578, 0.0578, 0.0578, 0.0578, 0.0578, 0.0578
], device='cuda:0')</code></pre>
      <p>âœ… Reduced extremes â†’ more stable gradients, better macro F1, fewer overfits.</p>
    </div>
  </div>
</div>

  <h3>ğŸ“Š Comparison Snapshot</h3>
  <table>
    <thead>
      <tr>
        <th>Class</th>
        <th># Train Images</th>
        <th>Sklearn Weight</th>
        <th>Log-Scaled Weight</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>person</td><td>632</td><td>0.1111</td><td>0.0179</td></tr>
      <tr><td>badger</td><td>38</td><td>1.8480</td><td>0.0574</td></tr>
      <tr><td>penguin</td><td>37</td><td>1.8979</td><td>0.0578</td></tr>
      <tr><td>wombat</td><td>37</td><td>1.8979</td><td>0.0578</td></tr>
      <tr><td>...</td><td>...</td><td>...</td><td>...</td></tr>
    </tbody>
  </table>

  <h4>Log Scaled Inverse Frequency Class Weights Diagram:</h4>
  <div style="text-align: center; margin-top: 30px;">
    <img src="pictures/logscaledinvfreqclassweights.png" alt="Class Distribution weights" style="max-width: 100%; height: auto;">
  </div>

  <h3>ğŸ§  Why This Worked</h3>
  <p>
    Log-scaling introduced smoother gradient behavior. It:
    <ul>
      <li>Softened the penalty for <code>person</code> while still acknowledging its dominance</li>
      <li>Balanced minority class contributions without over-amplifying noise</li>
      <li>Eliminated overfitting seen in the earlier weighted loss formulation</li>
      <li>Improved macro-F1 and rare class recall</li>
    </ul>
    <blockquote>
      <strong>Lesson Learned:</strong> Just because a method says â€œbalancedâ€ doesnâ€™t mean it actually balances well. Log-scaled class weights helped my model fairly see the long tail of my dataset â€” not just the majority class.
    </blockquote>
  </p>
</section>
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  <section id="cnn-architecture">
  <h2>â•°â•´ 8. ğŸ§± CNN Architecture Evolution & Mixed Precision Fixes</h2>
  <p>
    When it came time to define the actual model architecture, I iterated on a custom convolutional neural network built entirely from scratch using PyTorchâ€™s <code>nn.Sequential</code>. Both the capstone and final versions followed a traditional <span class="code-highlight">Conv â†’ BatchNorm â†’ Activation â†’ Pooling</span>
 stack, but the final iteration introduced a number of upgrades to improve training stability, generalization, and compatibility with mixed precision training.
  </p>

<div class="meanstd-comparison">
  <!-- Capstone Version -->
  <div class="window meanstd-box">
    <div class="title-bar">
      <div class="title-bar-text">ğŸ”§ Capstone Version: Basic But Brittle</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <p>The original model got the job done but left room for improvement. Notable limitations included:</p>
      <ul>
        <li>âŒ <code>ReLU</code> used in all layers, including fully connected layers (less forgiving with dead gradients)</li>
        <li>âŒ No <code>BatchNorm1d</code> layers after fully connected layers</li>
        <li>âŒ Static assignment of <code>out_classes</code></li>
        <li>âŒ Only one <code>Dropout(p=0.2)</code> layer â€” not enough regularization</li>
        <li>âŒ Vulnerable to mixed precision errors (e.g., <code>float16</code> tensors passed directly to <code>nn.Linear</code>)</li>
        <li>âœ… Did correctly use <code>AdaptiveAvgPool2d((1,1))</code> for flexible spatial size reduction</li>
      </ul>
      <p><strong>Total Parameters:</strong> 1,688,850<br>
         <strong>Trainable Parameters:</strong> 1,688,850</p>
    </div>
  </div>

  <!-- Final Version -->
  <div class="window meanstd-box">
    <div class="title-bar">
      <div class="title-bar-text">âœ… Final Version: Refined, Regularized, AMP-Compatible</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <p>The final model retained the same convolutional backbone but introduced a cleaner, more stable classifier head and better overall structure.</p>
      <ul>
        <li>âœ… Replaced <code>ReLU</code> with <code>GELU</code> in conv layers, and <code>LeakyReLU</code> in FC layers</li>
        <li>âœ… Reordered <code>BatchNorm1d</code> to come after activation in FC layers</li>
        <li>âœ… Added multiple <code>Dropout(p=0.3)</code> layers</li>
        <li>âœ… Introduced an intermediate bottleneck layer (512 â†’ 256)</li>
        <li>âœ… Dynamically calculated <code>out_classes</code></li>
        <li>âœ… Inserted custom <code>ToFloat32()</code> layer after flattening</li>
        <li>âœ… Tried <code>torch.compile()</code> for acceleration (left disabled)</li>
      </ul>
      <p><strong>Total Parameters:</strong> 1,953,042<br>
         <strong>Trainable Parameters:</strong> 1,953,042</p>
    </div>
  </div>
</div>


  <h3>ğŸ› Bug: <code>CUBLAS_STATUS_NOT_SUPPORTED</code></h3>
  <p>While enabling mixed precision (<code>autocast()</code>), the model initially crashed with a low-level CUDA error:</p>
  <pre><code>RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling cublasLtMatmulAlgoGetHeuristic</code></pre>
  <p><strong>Root cause:</strong><br>
  Passing a <code>float16</code> tensor into an <code>nn.Linear</code> layer without manually restoring it to <code>float32</code> precision â€” a common issue on consumer GPUs or certain CUDA driver versions.</p>

  <h3>ğŸ’¡ The Fix: Insert a Lightweight Float32 Patch</h3>
  <p>Since my model used <code>nn.Sequential</code> (no manual <code>forward()</code> method), I couldn't just write <code>x = x.float()</code> inline.</p>
  <p>Instead, I created a simple layer:</p>
  <pre><code class="language-python">class ToFloat32(nn.Module):
    def forward(self, x):
        return x.float()</code></pre>
  <p>Inserted just after flattening:</p>
  <pre><code class="language-python">nn.Flatten(),
ToFloat32(),  # ğŸ”¥ Converts float16 â†’ float32
nn.Linear(512, 512),</code></pre>
  <p>âœ… This preserved mixed precision elsewhere (e.g., convolutional layers) while preventing catastrophic crashes during fully connected passes.</p>

  <h3>ğŸ“Š Architecture Summary</h3>
  <table>
    <thead>
      <tr>
        <th>Feature</th>
        <th>Capstone Version</th>
        <th>Final Version</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Activation (Conv)</td>
        <td>ReLU</td>
        <td>GELU</td>
      </tr>
      <tr>
        <td>Activation (FC)</td>
        <td>ReLU</td>
        <td>LeakyReLU</td>
      </tr>
      <tr>
        <td>BatchNorm (FC)</td>
        <td>âŒ</td>
        <td>âœ… After Activation</td>
      </tr>
      <tr>
        <td>Dropout Layers</td>
        <td>1 Ã— p=0.2</td>
        <td>2 Ã— p=0.3</td>
      </tr>
      <tr>
        <td>Output Classes</td>
        <td>Hardcoded</td>
        <td>Dynamically computed</td>
      </tr>
      <tr>
        <td>Mixed Precision Safe</td>
        <td>âŒ</td>
        <td>âœ… via <code>ToFloat32()</code></td>
      </tr>
      <tr>
        <td>Total Parameters</td>
        <td>1,688,850</td>
        <td>1,953,042</td>
      </tr>
      <tr>
        <td>Trainable Parameters</td>
        <td>1,688,850</td>
        <td>1,953,042</td>
      </tr>
    </tbody>
  </table>

  <blockquote>
    <strong>ğŸ’­ Why This Matters:</strong>
    This architectural refactor wasnâ€™t just about stacking more layers. It was about fixing subtle bugs, improving model generalization, and enabling compatibility with hardware-level acceleration via mixed precision. The final model is performant, stable, and ready for real-world experimentation â€” all while being custom-built and lightweight enough to run on a laptop GPU.
  </blockquote>
</section>
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  <section id="weight-init">
  <h2>â•°â•´ 9. ğŸ§  Weight Initialization: Same Foundation, Still Solid</h2>
  <p>
    While many parts of the architecture evolved in the final version, my approach to weight initialization stayed consistent. I used a hybrid strategy tailored to the type of layer:
  </p>

  <ul>
    <li>âœ… <strong>Xavier Initialization</strong> for all fully connected (<code>nn.Linear</code>) layers â€” balances variance across layers and supports stable convergence.</li>
    <li>âœ… <strong>Kaiming (He) Initialization</strong> for convolutional layers (<code>nn.Conv2d</code>) â€” optimized for ReLU-like activations and preserves forward signal strength.</li>
  </ul>

  <details>
    <summary><strong>Code: Custom Weight Initialization Function</strong></summary>
    <pre><code class="language-python">def weights_init(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
        if m.bias is not None:
            nn.init.zeros_(m.bias)</code></pre>
  </details>

  <p>
    Although I experimented with alternate activation functions like <code>GELU</code>, I kept <code>nonlinearity='relu'</code> for Kaiming as a safe and compatible default.
  </p>

  <blockquote>
    <strong>ğŸ§  Insight:</strong> This function was called right after the model was defined using <code>net.apply(weights_init)</code> â€” ensuring reproducible and properly scaled initialization for each run.
  </blockquote>

  <p>
    âœ… This section didnâ€™t change between the capstone and final versions â€” and sometimes, thatâ€™s exactly what you want. If your foundation is solid, no need to fix what already works.
  </p>
</section>
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  <section id="hyperparameters">
  <h2>â•°â•´ 10. ğŸ§ª Hyperparameters, Custom Loss, and Optimizer Refinement</h2>

  <h3>ğŸ›ï¸ Core Hyperparameters (Capstone & Final)</h3>
  <p>
    I kept the training loop stable with a consistent base:
  </p>
  <pre><code class="language-python">epochs = 200
learning_rate = 0.001
weight_decay = 1e-5
accumulation_steps = 2</code></pre>

  <h3>ğŸ¯ From CrossEntropy to Custom SoftTarget Focal Loss</h3>
  <div class="loss-comparison">
  <div class="window loss-box">
    <div class="title-bar">
      <div class="title-bar-text">ğŸ”§ Capstone Version: Simple but Limited</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <p>Used standard <code>CrossEntropyLoss</code> with per-class weights:</p>
      <pre><code class="language-python">criterion = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))</code></pre>
      <ul>
        <li>âœ… Useful for class imbalance</li>
        <li>âŒ Didnâ€™t support soft labels from MixUp</li>
        <li>âŒ No modulation for hard/easy samples</li>
      </ul>
    </div>
  </div>

  <div class="window loss-box">
    <div class="title-bar">
      <div class="title-bar-text">ğŸš€ Final Version: SoftTargetFocalLoss</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <p>Custom-built loss function with full MixUp and class imbalance support:</p>
      <ul>
        <li>âœ… Class-specific weights (<code>alpha</code>)</li>
        <li>âœ… Soft label support</li>
        <li>âœ… Focal focusing (<code>gamma</code>)</li>
        <li>âœ… Optional label smoothing</li>
      </ul>
      <pre><code class="language-python">criterion = SoftTargetFocalLoss(
    alpha=class_weights_tensor.to(device),
    gamma=2.0,
    reduction='mean',
    label_smoothing=0.1,
    num_classes=out_classes
)</code></pre>
    </div>
  </div>
</div>


  <h3>âš™ï¸ Optimizer Upgrades: Smarter Weight Decay</h3>
  <h4>ğŸ”§ Capstone Version</h4>
  <pre><code class="language-python">optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=1e-4)</code></pre>
  <p>Applied weight decay to all parameters â€” including bias and batchnorm layers (suboptimal).</p>

  <h4>âœ… Final Version</h4>
  <p>Grouped parameters with and without decay, to better support <code>AdamW</code> best practices:</p>
  <pre><code class="language-python">def get_optimizer(model, lr, weight_decay):
    decay, no_decay = [], []
    for name, param in model.named_parameters():
        if "bias" in name or "bn" in name or "BatchNorm" in name:
            no_decay.append(param)
        else:
            decay.append(param)
    return torch.optim.AdamW([
        {'params': decay, 'weight_decay': weight_decay},
        {'params': no_decay, 'weight_decay': 0.0}
    ], lr=lr)</code></pre>

  <p>âœ… Excludes BatchNorm and bias terms from regularization â€” improving convergence and reducing over-penalization of critical parameters.</p>

  <h3>ğŸ“‰ Scheduler & Precision: More Targeted and Stable</h3>

  <h4>ğŸ§ª Capstone Version</h4>
  <pre><code class="language-python">scaler = torch.cuda.amp.GradScaler()
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', patience=2, factor=0.5, verbose=True)</code></pre>

  <h4>âœ… Final Version</h4>
  <pre><code class="language-python">scaler = torch.cuda.amp.GradScaler()
scheduler = ReduceLROnPlateau(
    optimizer, mode='max', factor=0.5, patience=5, verbose=True)</code></pre>

  <ul>
    <li>âœ… Switched from <code>mode='min'</code> to <code>mode='max'</code> to track validation accuracy instead of loss</li>
    <li>âœ… AMP scaler unchanged â€” crucial for efficient mixed precision training</li>
    <li>âœ… Tried other schedulers (like OneCycleLR), but this yielded more stable convergence</li>
  </ul>

  <h3>ğŸ” Learning Rate Scheduling: Tried Others, Stuck with the Winner</h3>
  <p>
    While tuning the final version of the training loop, I experimented with a few popular learning rate schedulers commonly suggested for imbalanced datasets, including:
  </p>
  <ul>
    <li><code>OneCycleLR</code></li>
    <li><code>CosineAnnealingLR</code></li>
    <li><code>CosineAnnealingWarmRestarts</code></li>
  </ul>
  <p>
    ğŸ“‰ <strong>OneCycleLR</strong> in particular is often favored for helping models navigate tough class imbalance scenarios. But in my case, it led to unstable training behavior â€” the <strong>macro F1 score</strong> would teeter and often stagnate, which isnâ€™t unusual for heavily imbalanced data like mine.
  </p>
  <p>
    ğŸ” Instead, I prioritized <strong>validation accuracy</strong> as my main performance indicator â€” which proved to be a more stable and meaningful signal throughout training. Based on that, I found that sticking with <code>ReduceLROnPlateau(mode='max')</code> delivered more reliable results and smoother convergence.
  </p>
  <p>
    âœ… <strong>Final Choice:</strong> <code>ReduceLROnPlateau</code> monitoring validation accuracy, with <code>patience=5</code> and <code>factor=0.5</code>.
  </p>

  <h3>ğŸ“ Metric: Macro F1 vs Accuracy</h3>
  <p>
    While macro F1 was tracked for completeness, I observed that F1 scores didn't always climb linearly â€” which is common with long-tailed class distributions. As a result, I leaned more on validation accuracy to drive scheduler behavior.
  </p>

  <blockquote>
    <strong>ğŸ§  Why This Matters:</strong> This phase of the pipeline wasnâ€™t just about picking numbers. It was about adapting your loss function, metric tracking, and optimization behavior to match the data and training style (MixUp, imbalance, class weights, soft labels). These changes led to smoother convergence and improved real-world performance.
  </blockquote>
</section>
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  <section id="training-loop">
  <h2>â•°â•´ 11. ğŸ§  Training the CNN: From â€œIt Worksâ€ to â€œIt Scalesâ€</h2>
  <p>
    While the capstone version was adequate and handled forward propagation, loss calculation, backward propagation and weight update well enough, the final version introduced a number of practical improvements that made training more stable, efficient, and easier to monitor.
  </p>

<div class="meanstd-comparison">
  <div class="window meanstd-box">
    <div class="title-bar">
      <div class="title-bar-text">ğŸ”§ Capstone Version: Functional, But Fragile</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <p>The original training loop did cover the essentials:</p>
      <ul>
        <li>âœ… Forward pass + loss computation</li>
        <li>âœ… Mixed precision with <code>autocast()</code> and <code>GradScaler()</code></li>
        <li>âœ… Accuracy, loss, and macro F1 logged per epoch</li>
        <li>âœ… Learning rate scheduling via <code>ReduceLROnPlateau(mode='min')</code></li>
      </ul>
      <p>But it missed important features that help with training longevity and reproducibility:</p>
      <ul>
        <li>âŒ No early stopping or checkpointing for best models</li>
        <li>âŒ No gradient accumulation to simulate larger batch sizes</li>
        <li>âŒ No protections for NaNs/Infs in the loss</li>
        <li>âŒ Memory wasnâ€™t explicitly cleared, which made crashes more likely</li>
        <li>âŒ Used hard labels only â€” not compatible with MixUp or soft-label loss functions</li>
      </ul>
    </div>
  </div>

  <div class="window meanstd-box">
    <div class="title-bar">
      <div class="title-bar-text">âœ… Final Version: Resilient, Efficient, and Metric-Savvy</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <p>The final loop received a full overhaul â€” tuned to support soft targets, protect training from edge cases, and gracefully handle GPU memory constraints.</p>
      <ul>
        <li>âœ… <strong>Gradient Accumulation</strong> to handle small batch sizes efficiently</li>
        <li>âœ… <strong>Gradient Clipping</strong> after <code>unscale()</code> to prevent exploding gradients</li>
        <li>âœ… <strong>NaN/Inf skip logic</strong> to prevent training from crashing</li>
        <li>âœ… <strong>Early Stopping</strong> with <code>patience=20</code> based on both validation accuracy and F1</li>
        <li>âœ… <strong>Dual Checkpointing</strong> â€” saves best model by val accuracy <em>and</em> best model by F1</li>
        <li>âœ… <strong>Soft Label Support</strong>: Converts to hard labels for accuracy metric</li>
        <li>âœ… <strong>Aggressive CUDA Memory Cleanup</strong> via <code>gc.collect()</code> and <code>torch.cuda.empty_cache()</code></li>
        <li>âœ… <strong>Learning Rate Logging</strong> per epoch for transparency</li>
        <li>âœ… <strong>Scheduler</strong> switched to <code>ReduceLROnPlateau(mode='max')</code></li>
        <li>âœ… <strong>Batch-level memory cleanup</strong> to prevent OOM</li>
        <li>âœ… <strong>Logs macro F1, precision, and recall</strong> every epoch</li>
      </ul>
    </div>
  </div>
</div>

  <div class="window" style="margin-top: 20px;">
  <div class="title-bar">
    <div class="title-bar-text">ğŸ§ª Example Enhancements Worth Highlighting</div>
    <div class="title-bar-controls">
      <button aria-label="Minimize"></button>
      <button aria-label="Close"></button>
    </div>
  </div>
  <div class="window-body">
  <h4>ğŸ” Gradient Accumulation + AMP</h4>
    <pre><code class="language-python"># Mixed precision and gradient accumulation
with autocast():  # AMP forward pass
    output = net(data)
    loss = criterion(output, label)

scaler.scale(loss).backward()
if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
    </code></pre>
	 <h4>ğŸ“Œ Checkpointing Logic</h4>
    <pre><code class="language-python"># Save best-performing model by F1 and accuracy
if valid_acc > best_val_acc + 1e-4:
    torch.save(net.state_dict(), "cap_model_weights_best.pth")
if f1 > best_val_f1 + 1e-4:
    torch.save(net.state_dict(), "best_model_by_f1.pth")
    </code></pre>
	<h4>â¹ï¸ Early Stopping</h4>
    <pre><code class="language-python"># Early stopping
if epoch_noimprove >= patience:
    print("Early stop no improvement in %d epochs." % patience)
    break
    </code></pre>
  </div>
</div>


  <h3>ğŸ“Š Why This Matters</h3>
  <p>
    The changes implemented allowed for better logging, improved memory handling on my limited hardware, and the ability to stop early once validation accuracy or F1 stopped improving. Saving checkpoints for both metrics gave me options for which version of the model best served my goals.
  </p>
  <p>
    This loop was designed to support real-world constraints: limited GPU memory, imbalanced classes, soft-label augmentation, and fluctuating validation metrics.
  </p>
</section>

  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  <section id="model-epochs-results">
  <h2>â•°â•´ 12. ğŸ“Š Capstone vs Final Model: Results</h2>
<p>
  Despite using the same image sources, the <strong>capstone model</strong> trained on a flawed version of the dataset due to an earlier bug in the
  dataset merging logic. As a result, the validation set was imbalanced and not representative â€” leading to poor generalization, low precision and recall,
  and heavy reliance on the <strong>"person"</strong> class. The <strong>final model</strong> corrected this issue by using a properly combined and balanced dataset,
  along with key enhancements like log-scaled inverse frequency class weighting, MixUp augmentation, and softmax threshold tuning.
  These changes led to more stable learning dynamics and significantly improved performance across all classes.
</p>

  <div class="meanstd-comparison" style="display: flex; gap: 20px;">
  <!-- Capstone Epoch Summary -->
  <div class="window meanstd-box" style="flex: 1;">
    <div class="title-bar">
      <div class="title-bar-text">ğŸŸ¡ Capstone: Epoch Summary</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body" style="max-height: 400px; overflow-y: auto; font-size: 13px;">
      <pre>
Epoch 1:  train loss 2.739, train acc 0.155, val loss 2.943, val acc 0.124
          precision 0.309, recall 0.142, F1-score 0.246
Epoch 10: train loss 2.047, train acc 0.351, val loss 2.694, val acc 0.321
          precision 0.217, recall 0.277, F1-score 0.264
Epoch 20: train loss 1.827, train acc 0.402, val loss 2.507, val acc 0.394
          precision 0.255, recall 0.318, F1-score 0.259
Epoch 30: train loss 1.691, train acc 0.467, val loss 2.481, val acc 0.417
          precision 0.278, recall 0.337, F1-score 0.278
Epoch 40: train loss 1.627, train acc 0.476, val loss 2.465, val acc 0.434
          precision 0.290, recall 0.343, F1-score 0.286
Epoch 50: train loss 1.709, train acc 0.457, val loss 2.458, val acc 0.434
          precision 0.291, recall 0.339, F1-score 0.282
Epoch 60: train loss 1.716, train acc 0.458, val loss 2.473, val acc 0.425
          precision 0.287, recall 0.340, F1-score 0.283
Epoch 70: train loss 1.681, train acc 0.469, val loss 2.457, val acc 0.428
          precision 0.279, recall 0.336, F1-score 0.278
Epoch 75: train loss 1.699, train acc 0.455, val loss 2.484, val acc 0.423
          precision 0.281, recall 0.335, F1-score 0.277

â†’ Final F1-score (Val): 0.277
      </pre>
    </div>
  </div>

  <!-- Final Model Epoch Summary -->
  <div class="window meanstd-box" style="flex: 1;">
    <div class="title-bar">
      <div class="title-bar-text">ğŸŸ  Final Model: Epoch Summary</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body" style="max-height: 400px; overflow-y: auto; font-size: 13px;">
      <pre>
Epoch 1:  train loss 0.148, train acc 0.114, val loss 0.147, val acc 0.141
          precision 0.409, recall 0.108, F1-score 0.335
Epoch 10: train loss 0.105, train acc 0.334, val loss 0.080, val acc 0.398
          precision 0.244, recall 0.300, F1-score 0.349
Epoch 20: train loss 0.091, train acc 0.441, val loss 0.076, val acc 0.505
          precision 0.299, recall 0.344, F1-score 0.448
Epoch 30: train loss 0.085, train acc 0.502, val loss 0.062, val acc 0.617
          precision 0.357, recall 0.437, F1-score 0.437
Epoch 40: train loss 0.074, train acc 0.605, val loss 0.059, val acc 0.659
          precision 0.448, recall 0.492, F1-score 0.507
Epoch 50: train loss 0.069, train acc 0.652, val loss 0.059, val acc 0.669
          precision 0.438, recall 0.494, F1-score 0.501
Epoch 60: train loss 0.067, train acc 0.691, val loss 0.057, val acc 0.698
          precision 0.482, recall 0.515, F1-score 0.486
Epoch 70: train loss 0.066, train acc 0.716, val loss 0.056, val acc 0.734
          precision 0.526, recall 0.555, F1-score 0.524
Epoch 75: train loss 0.064, train acc 0.695, val loss 0.056, val acc 0.732
          precision 0.516, recall 0.553, F1-score 0.523

â†’ Final F1-score (Val): 0.523
      </pre>
    </div>
  </div>
</div>
	<hr style="width: 60%; margin: 40px auto; border: none; border-top: 1px solid #c0c0c0;">
  <h3>ğŸ“‹ Model Performance: Test Set Results</h3>

  <div class="meanstd-comparison" style="margin-top: 20px;">
    <div class="window meanstd-box" style="flex: 1;">
      <div class="title-bar">
        <div class="title-bar-text">ğŸ§ª Capstone Model - Test Set</div>
        <div class="title-bar-controls">
          <button aria-label="Minimize"></button>
          <button aria-label="Close"></button>
        </div>
      </div>
      <div class="window-body" style="font-size: 13px;">
        <ul>
          <li>ğŸ“‰ Accuracy: <strong>39.6%</strong></li>
          <li>ğŸ”§ Avg Loss: <code>2.6282</code></li>
          <li>ğŸ“Š Weighted F1: <strong>0.45</strong></li>
          <li>ğŸ“Š Macro F1: <strong>0.23</strong></li>
          <li>ğŸ¤– Dominant Class: <code>"person"</code></li>
        </ul>
        <p><strong>Confusion Matrix:</strong> <em>See below</em></p>
        <p><strong>Classification Report:</strong> Imbalanced, underperforming on most minority classes.</p>
      </div>
    </div>

    <div class="window meanstd-box" style="flex: 1;">
      <div class="title-bar">
        <div class="title-bar-text">âœ… Final Model - Test Set</div>
        <div class="title-bar-controls">
          <button aria-label="Minimize"></button>
          <button aria-label="Close"></button>
        </div>
      </div>
      <div class="window-body" style="font-size: 13px;">
        <ul>
          <li>ğŸ“ˆ Accuracy: <strong>71.7%</strong></li>
          <li>ğŸ”§ Avg Loss: <code>0.0561</code></li>
          <li>ğŸ“Š Weighted F1: <strong>0.73</strong></li>
          <li>ğŸ“Š Macro F1: <strong>0.54</strong></li>
          <li>ğŸ§  Balanced across all 18 classes</li>
        </ul>
        <p><strong>Confusion Matrix:</strong> <em>See below</em></p>
        <p><strong>Classification Report:</strong> Stronger precision and recall for all classes.</p>
      </div>
    </div>
  </div>
  
  <hr style="width: 60%; margin: 40px auto; border: none; border-top: 1px solid #c0c0c0;">
  <h4>ğŸ“Š Confusion Matrices: Visual Comparison â€“ Capstone vs Final Model</h4>
<!-- ğŸ“Š Confusion Matrices: Side-by-Side Comparison -->
<div style="display: flex; gap: 20px; flex-wrap: wrap; justify-content: center; margin-top: 20px;">

  <!-- Capstone Confusion Matrix Box -->
  <div class="window" style="width: 48%;">
    <div class="title-bar">
      <div class="title-bar-text">ğŸ“Š Capstone Model â€” Confusion Matrix</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <p style="text-align: center; font-weight: bold;">ğŸŸ¡ Raw Confusion Matrix</p>
      <img src="pictures/cap_conf_matrix.png" alt="Capstone Confusion Matrix" style="width: 90%; display: block; margin: 0 auto;">
      <p style="text-align: center; font-weight: bold;">ğŸŸ¡ Normalized Confusion Matrix</p>
      <img src="pictures/cap_confnorm_matrix.png" alt="Capstone Normalized Confusion Matrix" style="width: 90%; display: block; margin: 0 auto;">
    </div>
  </div>

  <!-- Final Confusion Matrix Box -->
  <div class="window" style="width: 48%;">
    <div class="title-bar">
      <div class="title-bar-text">ğŸ“Š Final Model â€” Confusion Matrix</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body">
      <p style="text-align: center; font-weight: bold;">ğŸŸ  Raw Confusion Matrix</p>
      <img src="pictures/final_conf_matrix.png" alt="Final Confusion Matrix" style="width: 90%; display: block; margin: 0 auto;">
      <p style="text-align: center; font-weight: bold;">ğŸŸ  Normalized Confusion Matrix</p>
      <img src="pictures/final_confnorm_matrix.png" alt="Final Normalized Confusion Matrix" style="width: 90%; display: block; margin: 0 auto;">
    </div>
  </div>

</div>
  <hr style="width: 60%; margin: 40px auto; border: none; border-top: 1px solid #c0c0c0;">
<!-- F1 Score Comparison Container -->
<h4>ğŸ§  Deep Dive: F1-Score Comparison â€“ Capstone vs Final Model</h4>
<div style="display: flex; flex-direction: column; align-items: center; gap: 20px; margin-top: 20px;">

  <!-- ğŸ“ˆ F1-Score Evolution (Early Epochs) -->
  <div class="window" style="max-width: 700px;">
    <div class="title-bar">
      <div class="title-bar-text">ğŸ“ˆ F1-Score Evolution (Early Epochs)</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body" style="text-align: center;">
      <img src="pictures/capvsfinalmodel.png" alt="F1 Score Comparison Early" style="width: 650px; height: auto; margin-bottom: 10px;">
      <ul style="text-align: left; font-size: 13px; margin-top: 5px;">
        <li><strong>Capstone Model (Dashed Yellow):</strong> Starts low, peaks under 0.30, plateaus quickly.</li>
        <li><strong>Final Model (Solid Orange):</strong> Starts stronger, climbs to 0.42+ with more stable training.</li>
      </ul>
    </div>
  </div>
</dev>
<!-- ğŸ§  F1 Explanation + Interpretation + Why It Matters -->
<div style="max-width: 800px; margin: 40px auto; text-align: left;">
  <h4>âš–ï¸ What is F1-Score and Why It Matters?</h4>
  <ul>
    <li><strong>F1-score</strong> is the harmonic mean of <em>precision</em> & <em>recall</em>.</li>
    <li>It's especially crucial in <strong>imbalanced classification problems</strong>, like yours, where some classes (â€œ<code>person</code>â€) vastly outnumber others (animals).</li>
    <li><strong>F1</strong> ensures a balanced view: A model must correctly identify positive examples and avoid false positives to do well.</li>
  </ul>

  <hr style="border-top: 1px solid #c0c0c0;">
  <h4>ğŸ“‰ Interpretation / Changes</h4>
  <ul>
    <li><strong>Capstone Model:</strong><br>
      This curve reflects slow, unstable learning, likely because the model is overfitting to the majority class (â€œ<code>person</code>â€) and struggling to generalize to the underrepresented animal classes.</li>
    <li><strong>Final Model:</strong><br>
      Slight dip at epochs 4â€“5, but performance stays consistently high (above 0.38), indicating a resilient learning process.</li>
  </ul>

  <hr style="border-top: 1px solid #c0c0c0;">
  <h4>âœ¨ Why This Matters</h4>
  <ul>
    <li><strong>This early F1-score boost is critical.</strong> In real-world training:</li>
    <li>Models that learn faster with better generalization early on are typically more stable and performant overall.</li>
    <li>The Final modelâ€™s trajectory indicates it has learned to recognize <strong>all 18 classes</strong> early â€” not just the dominant one.</li>
  </ul>
</div>

  <!-- ğŸ“Š F1-Score Evolution (30 Epochs) -->
  <div class="window" style="max-width: 700px;">
    <div class="title-bar">
      <div class="title-bar-text">ğŸ“Š F1-Score Evolution (30 Epochs)</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body" style="text-align: center;">
      <img src="pictures/f1evolutioncapfinalmodel.png" alt="F1 Score Comparison Full Epochs" style="width: 650px; height: auto; margin-bottom: 10px;">
      <ul style="text-align: left; font-size: 13px; margin-top: 5px;">
        <li><strong>Capstone:</strong> Low and choppy growth. Peaks early, plateaus fast.</li>
        <li><strong>Final:</strong> Steady climb from ~0.33 to ~0.51. Less fluctuation, more learning stability.</li>
      </ul>
    </div>
  </div>
</div>
<div style="max-width: 800px; margin: 40px auto; text-align: left;">
  <h4>ğŸ§  Key Observations:</h4>
  <ul>
  <li><strong>Capstone Model:</strong>
    <ul>
      <li>Starts around 0.25, creeps slowly upwards.</li>
      <li>Mostly fluctuates in the 0.26â€“0.30 range for 30 epochs.</li>
      <li>Peaks early (Epoch 4) and plateaus quickly, reflecting:</li>
      <li>Inability to improve generalization.</li>
      <li>Over-reliance on the â€œ<code>person</code>â€ class.</li>
      <li>Vanishing gradient effects possibly due to poor class representation.</li>
    </ul>
  </li>
</ul>
<ul>
  <li><strong>Final Model:</strong>
    <ul>
      <li>Starts at 0.335, grows steadily and crosses 0.50 by Epoch 15.</li>
      <li>Maintains stable high performance above 0.48 from Epoch 20 onward.</li>
      <li>Indicates:</li>
      <ul>
      <li>Robust learning from underrepresented classes.</li>
      <li>Clear benefit from weighting and possibly augmentation.</li>
      <li>Balanced gradient propagation even in later stages of training.</li>
      </ul>
    </ul>
  </li>
</ul>

<p>This suggests that the Final model not only learned faster, but sustained meaningful learning across all classes â€” a direct result of balancing strategies and refined architecture choices. This long-term improvement is what makes the Final model truly production-ready.</p>
</div>

  <hr style="width: 60%; margin: 40px auto; border: none; border-top: 1px solid #c0c0c0;">
  <h4>ğŸ” Metric Comparison + Summary</h4>
<!-- ğŸ” Metric Comparison + Summary Side-by-Side -->
<div class="meanstd-comparison" style="display: flex; gap: 20px; margin-top: 20px;">
  <!-- Metric Comparison -->
  <div class="window meanstd-box" style="flex: 1;">
    <div class="title-bar">
      <div class="title-bar-text">ğŸ” Metric Comparison</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body" style="font-size: 13px;">
      <table style="width: 100%; border-collapse: collapse;" border="1">
        <thead style="background-color: #dcdcdc;">
          <tr>
            <th>Metric</th>
            <th>Capstone Model</th>
            <th>Final Model</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>Test Accuracy</td><td>0.396</td><td><strong>0.717 âœ…</strong></td></tr>
          <tr><td>Weighted F1</td><td>0.45</td><td><strong>0.730 âœ…</strong></td></tr>
          <tr><td>Macro F1</td><td>0.23</td><td><strong>0.540 âœ…</strong></td></tr>
          <tr><td>Recall (avg)</td><td>0.29</td><td><strong>0.570 âœ…</strong></td></tr>
          <tr><td>Precision (avg)</td><td>0.22</td><td><strong>0.580 âœ…</strong></td></tr>
        </tbody>
      </table>
    </div>
  </div>

  <!-- Summary -->
  <div class="window meanstd-box" style="flex: 1;">
    <div class="title-bar">
      <div class="title-bar-text">âœ… Summary</div>
      <div class="title-bar-controls">
        <button aria-label="Minimize"></button>
        <button aria-label="Close"></button>
      </div>
    </div>
    <div class="window-body" style="font-size: 13px;">
      <ul>
        <li><strong>+33%</strong> boost in test accuracy</li>
        <li><strong>+30%</strong> improvement in macro F1</li>
        <li>Balanced representation across all 18 classes</li>
        <li>â€œPersonâ€ class no longer dominates model behavior</li>
      </ul>
    </div>
  </div>
  </div>
</section>  
  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
<section id="cnn-imbalance-summary" style="margin-top: 60px;">
  <h2>â•°â•´ 13. ğŸ§  Blog Summary: Building a Multi-Class CNN from Scratch with Severe Class Imbalance</h2>
      <p>In this project, I engineered an adequate convolutional neural network (CNN) entirely from scratch using PyTorch, embracing the unique challenge of a severely imbalanced, multi-class classification problem. It was a lot of fun disecting my original problem line by line and debugging it in its entirety. I was able to learn and apply a myriad of techniques, seeing what impact each one had. All in all it gave me a better understanding of creating an ML pipeline and reiterated to me the importance of making sure the data is properly preprocessed before starting.</p>

  <h3>ğŸ› ï¸ Project Evolution: From Capstone to Final Version</h3>
  <p>
    The journey began with a capstone project built on AWS SageMaker, but later migrated to my local laptop (Alienware m15 R7) due to cost and practicality. This shift demanded better memory management and smarter computation strategies, including frequent CUDA cache clearing and modular notebook restructuring.
  </p>

  <h3>ğŸ§¹ Dataset Debugging & Cleanup</h3>
  <p>The original capstone had major flaws:</p>
  <ul>
    <li>Misaligned class indices during merging caused misclassification.</li>
    <li>Empty folders and inconsistent labels led to poor training quality.</li>
    <li>Redundant transforms and loader bugs hampered performance.</li>
  </ul>
  <p>The final version fixed all of this by:</p>
  <ul>
    <li>Applying label offsets during merging,</li>
    <li>Verifying dataset structure programmatically,</li>
    <li>Saving cleaned, reproducible datasets with JSON summaries and stats.</li>
  </ul>

  <h3>ğŸ§® Key Technical Improvements</h3>
  <ul>
    <li>Custom class weighting using a log-scaled inverse frequency approach to better handle imbalance, replacing overcompensating sklearn weights.</li>
    <li>Advanced normalization using double-pass mean & std computations with caching.</li>
    <li>MixUp augmentation to introduce soft-label training and combat overfitting.</li>
    <li>A more stable, AMP-compatible architecture with GELU/LeakyReLU activations, multiple Dropout layers, and custom layers for float32 conversion.</li>
  </ul>

  <h3>ğŸ“‰ From â€œFunctionalâ€ to â€œEfficientâ€: Training Loop Refinement</h3>
  <p>The training pipeline now includes:</p>
  <ul>
    <li>Gradient accumulation for batch efficiency,</li>
    <li>Early stopping and dual checkpointing for val accuracy and F1,</li>
    <li>Focal loss with soft labels and class weighting,</li>
    <li>Memory safety mechanisms to prevent CUDA crashes.</li>
  </ul>

  <h3>ğŸ“Š Model Comparison: Capstone vs Final</h3>
  <table border="1" style="width: 100%; border-collapse: collapse; margin-bottom: 10px;">
    <thead style="background-color: #dcdcdc;">
      <tr>
        <th>Metric</th>
        <th>Capstone Model</th>
        <th>Final Model</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>Validation F1</td><td>0.277</td><td>0.523</td></tr>
      <tr><td>Test Accuracy</td><td>39.6%</td><td>71.7%</td></tr>
      <tr><td>Macro F1 (Test)</td><td>0.23</td><td>0.54</td></tr>
      <tr><td>Weighted F1 (Test)</td><td>0.45</td><td>0.73</td></tr>
      <tr><td>Dominant Class Bias</td><td>High ("person")</td><td>Balanced</td></tr>
    </tbody>
  </table>
  <p>
    The final model exhibited drastically improved precision, recall, and generalization, especially on minority classes, due to smarter loss formulation, augmentation, and architectural tweaks.
  </p>

  <h3>ğŸš€ Why It Matters</h3>
  <p>
    This wasnâ€™t just about getting high scores. It was about demonstrating deep ML engineering under resource constraints, with dirty, real-world data, and pushing custom solutions from scratch instead of relying on pretrained shortcuts.
  </p>
</section>

  <hr style="margin: 40px 0; border: none; border-top: 2px inset black;">
  
</div>

